---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Mixed Effects or Multilevel Modeling {#module-24}

## Objectives

> In this module, we extend our discussion of regression modeling even further to include "mixed effects" or "multilevel" models.

## Preliminaries

- Install and load this package in ***R***: [{lme4}](https://cran.r-project.org/web/packages/lme4/lme4.pdf)
- Load {tidyverse}

```{r}
#| include: false
library(tidyverse)
library(lme4)
```

## Overview of "Mixed" Models

A final extension of linear regression modeling that we will talk about is so-called "multilevel" or "mixed effects" modeling. This is a very complex topic, and we will only scratch the surface!

In a (general or generalized) linear mixed model, we have a reponse variable, $Y$, and observations that fall into different factor categories each with some set of levels (e.g., "sex" with levels "male" and "female"), and we are interested in the effects of the various factors **and** factor levels on the response variable. Generically, if $\mu$ = a population mean response and $\mu_A$ = mean response for observations belonging to factor level A, then the effect of A is given by $\mu$ - $\mu_A$. We have already dealt with factors and factor levels in our linear regression models when we looked at categorical predictors (e.g., *sex*, *rank category*) in our discussions of ANOVA and ANCOVA.

We can conceptualize these factor effects as being either *fixed* or *random*. *Fixed* factors are those that reflect all levels of interest in our study, while *random* effects are those that represent only a sample of the levels of interest. For example, if we include **sex** as a factor in a model with the factor levels "male" and "female", this (typically) will cover the entire gamut of levels of interest our study, thus we would consider **sex** a fixed factor. When we were doing ANOVA and ANCOVA analyses previously, we were implicitly looking at the effects of such fixed factors.

However, if our observational data were to consist of repeated observations of the same sampling unit, e.g., measurements taken on the same set of individuals on different dates, **individual ID** would be considered a random factor because it is unlikely that we will have collected data from all possible "levels of interest", i.e., from all possible individual subjects. We have not yet dealt with such random factors as an additional source of variance in our modeling.

So-called **mixed models**, then, are those that include BOTH fixed and random effects. Including random effects in addition to fixed effects in our models has several ramifications:

- Using random effects broadens the scope of inference. That is, we can use statistical methods to infer something about the population from which the levels of the random factor have been drawn.
- Using random effects naturally incorporates dependence in the model and helps us account for pseudoreplication in our dataset. Observations that share the same level of the random effects are explicitly modeled as being correlated. This makes mixed effect modeling very useful for dealing with time series data, spatially correlated data, or situations where we have repeated observations/measures from the same subjects or sampling unit.
- Using random factors often gives more accurate parameter estimates.
- Incorporating random factors, however, does require the use of more sophisticated estimation and fitting methods.

Not surprisingly, there several different varieties of mixed modeling approaches, which are supported in a variety of different packages in ***R***:

- Standard or general **Linear Mixed Models (LMM)**, analogous to standard or general linear regression - used when we are dealing with normally distributed variables and error structures.
- **Generalized Linear Mixed Models (GLMM)**, analogous to generalized linear regresseion - used when we are dealing with various other variable types and error structure (e.g., binary, proportion, or count data).
- **Nonlinear Mixed Models (NLMM)**, analogous to nonlinear regression - used if we are dealing with situations where our response variable is best modeled by a nonlinear combination of predictor variables.

> **NOTE:** We have not talked at all in this course about general or generalized NONLINEAR modeling, but it is worth knowing that such approaches are also possible. NONLINEAR modeling is where our regression equation is a **nonlinear** function of the model parameters.

We will explore "mixed effects" modeling using an example based on this [excellent tutorial](http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf).

### EXAMPLE: {.unnumbered}

Suppose we have measured the amount of grooming received by female chimpanzees when they are either in their periovulatory period (i.e., the window of 2-3 days around the likely time of ovulation) or duing other portions of their reproductive cycle. We collected data on the **duration of grooming bouts received** and scored a female's **reproductive condition** at the time as a categorical factor with two levels: "POP" versus "NONPOP". On top of that, we also recorded data on female parity at the time of the grooming bout, i.e., whether the female had given birth previously (was "parous", or "P") or had not yet had an offspring (was "nulliparous", or "N").

If we are interested in how reproductive condition and parity influence how much grooming a female receives, our simple regression model would look like this:

$$grooming\ duration \sim reproductive\ condition + parity + \epsilon$$

Also imagine that our study design was such that we took multiple observations per subject. That is, our data set includes records of multiple grooming bouts received by each subject. This situation violates the assumption of independence of observations that we make for standard linear regression: multiple responses/measures from the same subject cannot be regarded as independent from each other.

Using a mixed effects model, we can deal with this situation by adding **subject ID** as a random effect in our model. Doing so allows us to address the nonindependence issue by estimating a different set of parameters for each level of the factor "subject". We can either estimate a different *intercept* for each subject (which would correspond to each female having a different "baseline" level of grooming received) or estimate a different *intercept* **and** *slope* (where individual subjects are presumed to differ both in the baseline level of grooming received and the strength of the relationship between grooming duration, on the one hand, and reproductive condition and parity, on the other). Our mixed effects model estimates these individual level parameters in addition to the main effects of each variable.

This is why a "mixed effects" model is called a **mixed** model. The models that we have considered so far have been **fixed effects only** models and included only one or more "fixed" predictor variables and a general error term. We essentially divided the world into things that we somehow understand or that are systematic (the fixed effects, or the explanatory variables) and things that we could not control for or do not understand (the general error, or $\epsilon$). These fixed effects models did not examine possible structure *within* the error term.

In a mixed model, by contrast, we add one or more random effects to our fixed effects that may explain a portion of the variance in our error term.

Let's explore these idea using some actual data. First, load in the dataset "chimpgrooming.csv" and do some exploratory data analysis:

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/chimpgrooming.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
summary(d)
```

### CHALLENGE {.unnumbered}

Plot grooming received duration in relation to subject ID...

```{r}
#| code-fold: true
(p <- ggplot(data=d, aes(x=subject, y=duration)) +
   geom_boxplot() + xlab("subject") +
   theme(axis.text.x = element_text(angle = 90)))
```

Now plot grooming received in relation to reproductive condition...

```{r}
#| code-fold: true
(p <- ggplot(data = d, aes(x = reprocondition, y = duration)) +
  geom_boxplot() + xlab("reproductive condition") +
  theme(axis.text.x = element_text(angle = 90)))
```

Now plot grooming received in relation to reproductive condition *and* parity...

```{r}
#| code-fold: true
(p <- ggplot(data = d, aes(x = reprocondition, y = duration,
  fill=factor(parity))) + geom_boxplot() +
  xlab("reproductive condition") +
  labs(fill = "parity") +
  theme(axis.text.x = element_text(angle = 90)))

# or

(p <- ggplot(data = d, aes(x = parity, y = duration,
  fill=factor(reprocondition))) + geom_boxplot() +
  xlab("parity") +
  labs(fill = "repro\ncondition") +
  theme(axis.text.x = element_text(angle = 90)))
```

From these plots, we can see lots of [a] individual variation in how much grooming is received, [b] variation in response to reproductive condition, [c] limited variation due to parity, and [d] variation in response to this combination of fixed effects.

Finally, plot grooming received into relation to both a fixed effect (reprocondition) and a random effect (subject)...

```{r}
#| code-fold: true
(p <- ggplot(data = d, aes(x = reprocondition, y = duration,
  fill = factor(subject))) +
  geom_boxplot() +
  xlab("reproductive condition") +
  labs(fill = "subject") +
  theme(axis.text.x = element_text(angle = 90)))

# or

(p <- ggplot(data = d, aes(x = subject, y = duration,
  fill = factor(reprocondition))) +
  geom_boxplot() +
  xlab("subject") +
  labs(fill = "repro\ncondition") +
  theme(axis.text.x = element_text(angle = 90)))
```

What patterns do you see? There is, again, lots of apparent variation!

## Random Intercept Models

We will now perform an initial mixed effects analysis where we look at how reproductive condition and parity (as fixed effects) affect grooming duration, where we include individual subject ID as a random effect.

Here is a first mixed effects model that we will fit, using one extension of formula notation that is commonly used in ***R***.

$$grooming\ duration \sim reproductive\ condition + parity + (1|subject) + \epsilon$$

Here, the 1 refers to the fact that we want to estimate an *intercept* and the pipe "\|" operator following the "1" signifies that we want to estimate a *different intercept for each subject*. Note that this generic formula still contains a general error term, $\epsilon$, to highlight that there will still be unexplained "error" variance after accounting for both fixed and random effects in the model.

We can think of this formula as saying that we expect our dataset to include multiple observations of the response variable per subject, and these responses will depend, in part, on each subject's baseline level. This effectively accounts the nonindependence that stems from having multiple responses by the same subject.

The {lme4} package in **R** is commonly used for mixed effects modeling, and the function `lmer()` is the mixed model equivalent of the function `lm()`. In the formula syntax for mixed effects models using the {lme4} package, fixed effects are included without parentheses while random effects are included in parentheses (the error, $\epsilon$, is understood and is not included explicitly).

> **NOTE:** We could also use the package {nlme} for mixed effects modeling (which requires a slightly different formula syntax than that used here). That same package also allows us to do **nonlinear** mixed effects modeling, which we will not be talking about. It is important to note that {lme4} uses, by default, a slightly different parameter estimation algorithm than {nlme}. Unless otherwise specified, {lme4} uses "restricted maximum likelihood" (REML) rather than ordinary maximum likelihood estimation, which is what is used in {nlme}. In practice, these give very similar results. We will see below that when we want to compare different models using {lme4}, we will need to tell {lme4} to use ordinary maximum likelihood.

The code block below implements this first "mixed effects" model:

```{r}
lme <- lmer(data = d, duration ~ reprocondition + parity +
  (1 | subject))
summary(lme)
```

Let's focus on the output for the random effects first. Have a look at the column standard deviation. The entry for *subject* shows us how much variability in grooming duration (apart from that explained by the fixed effects) is due to subject ID. The entry for *Residual* summarizes the remaining variability in grooming duration that is not due to *subject* or to our fixed effects. This is our $\epsilon$, the "random" deviations from the predicted values that are not due to either subject or our fixed effects.

The fixed effects output mirrors the coefficient tables that we have seen previously in our linear models that have focused only on fixed effects. The coefficient "reproconditionPOP" is the $\beta$ coefficient (slope) for the categorical effect of reproductive condition. The positive sign for the coefficient means that grooming duration is GREATER by 20.293 units for POP than for NONPOP females. Then, there's a standard error associated with this slope, and a t value, which is simply the estimate divided by the standard error.

The coefficient "parityP" is the $\beta$ coefficient for the categorical effect of parity. The grooming duration associated with being parous versus nulliparous is GREATER by 109.65 units.

The INTERCEPT in this case is the grooming duration associated with being an average, nulliparous, NONPOP female. Like the `lm()` function, the `lmer()` took whatever factor level came first in the alphabet to be the reference level for each fixed effect variable.

Let's also look at the coefficients coming out of the model.

```{r}
coefficients(lme)
```

We can see the separate intercepts, or "baseline" level of grooming received, associated with each female when they are (presumably) nulliparous and in a NONPOP reproductive condition.

> **NOTE:** Not all females are, necessarily, ever seen in a particular parity or reproductive condition!

### Inference using LRTs {.unnumbered}

In mixed effects models, it is not as straightforward as it is for standard linear models to determine p values associated with either overall models or individual coefficients. However, using **likelihood ratio tests**, which we previously used for comparing among generalized linear models, is one common approach. Likelihood is the probability of seeing the data we have actually collected *given* a particular model. The logic of the likelihood ratio test is to compare the likelihood of two models with each other, i.e., a model that includes the factor that we are interested in versus a reduced, nested model with that factor excluded.

So... if we are interested in the effect of **reproductive condition** on grooming duration, we could compare a more complex model...

$$grooming\ duration \sim reproductive\ condition + parity + (1|subject) + \epsilon$$

to a nested, less complex model...

$$grooming\ duration \sim parity + (1|subject) + \epsilon$$

In ***R***, we would do this as follows:

```{r}
full <- lmer(data = d, duration ~ reprocondition + parity +
  (1 | subject), REML = FALSE)
# note the additional `REML=` argument
summary(full)
reduced <- lmer(data = d, duration ~ parity +
  (1 | subject), REML = FALSE) 
summary(reduced)
```

> **NOTE:** Here, we added the argument `REML=FALSE` to the `lmer()` function. This is necessary to do when we want to compare models using the likelihood ratio test. Basically, REML uses a different algorithm to determine likelihood values than ordinary likelihood, and, if we want to use these likelihoods to execute an LRT, we need to use ordinary likelihood. See [this site](http://users.stat.umn.edu/~gary/classes/5303/handouts/REML.pdf) for a more complete explanation of this issue.

We then perform the likelihood ratio test using the `anova()` function or the `lrtest()` function from {lmtest}.

```{r}
#| message: false
#| warning: false
anova(reduced, full)
library(lmtest)
lrtest(reduced, full)
detach(package:lmtest)
```

These results tell us that the model containing both **reproductive condition** and **parity** as *fixed* effects fits the data better than a model lacking **reproductive condition** and containing only **parity** as a *fixed* effect, while accounting for **subject** as a *random* effect.

### CHALLENGE {.unnumbered}

Now, compare a model containing **reproductive condition** and **parity** to one containing just **reproductive condition**.

```{r}
#| code-fold: true
full <- lmer(data = d, duration ~ reprocondition + parity +
  (1|subject), REML=FALSE)
reduced <- lmer(data=d, duration~reprocondition +
  (1|subject), REML=FALSE)
anova(reduced, full)
```

Based on this result, including **parity** as well as **reproductive condition** as fixed effects also significantly improves the fit of our model.

### CHALLENGE {.unnumbered}

Construct a model that includes an interaction of **reproductive condition** and **parity** and compare it to a model without the interaction term. Is the interaction of these two fixed effects significant?

```{r}
#| code-fold: true
full <- lmer(data = d, duration ~ reprocondition * parity +
  (1 | subject), REML = FALSE)
reduced <- lmer(data = d, duration ~ reprocondition + parity +
  (1 | subject), REML=FALSE)
anova(reduced, full)
```

In this case, adding the interaction of **reproductive condition** and **parity** does not significantly improve the explanatory power of the model.

## Random Slope Models

In the exercise above, we included only estimation of a separate INTERCEPT for each female and presumed that the same relationships (SLOPES) between grooming duration and reproductive condition + parity obtained for all females. But we can also allow that relationship to vary from subject to subject. We would indicate this model in formula notation as follows:

$$grooming\ duration \sim reproductive\ condition + parity\ +$$ $$(1 + reproductive\ condition|subject) + (1 + parity|subject) + \epsilon$$

```{r}
#| message: false
#| warning: false
lme <- lmer(data = d, duration ~ reprocondition + parity +
  (1 + reprocondition | subject) + (1 + parity | subject),
  REML = FALSE)
summary(lme)
```

Here, we have changed the random effects, which now look a little more complicated. The notation "(1 + reprocondition \| subject)" tells the model to estimate differing baseline levels of grooming duration (the intercept, represented by 1) as well as differing responses to the main factor in question, which is reproductive condition in this case. We do the same for parity.

Looking at the coefficients of the new model, we see the effects. Each female now has a different intercept **and** a different coefficient for the slopes of grooming duration as a function of both reproductive condition and parity.

```{r}
coefficients(lme)
```

### Inference using LRTs {.unnumbered}

To then get p values associated with each of the fixed factors, we can use likelihood ratio tests...

```{r}
#| message: false
#| warning: false
# random factors only
null <- lmer(data = d, duration ~ (1 + reprocondition | subject) +
  (1 + parity | subject), REML = FALSE)

# full model with both fixed effects
full <- lmer(data = d, duration ~ reprocondition + parity +
  (1 + reprocondition | subject) + (1 + parity | subject),
  REML = FALSE)

# model without reproductive condition as a fixed effect
minusRC <- lmer(data = d, duration ~ parity +
  (1 + reprocondition | subject) + (1 + parity | subject),
  REML = FALSE)

# model without parity as a fixed effect
minusP <- lmer(data = d, duration ~ reprocondition +
  (1 + reprocondition | subject) + (1 + parity | subject),
  REML = FALSE)

# p value for reproductive condition as a fixed effect
anova(minusRC, full)

# p value for parity as a fixed effect
anova(minusP, full)
```

### Inference using AIC {.unnumbered}

For a long time, the appropriateness of our mixed models was assessed as above - i.e., by evaluating the significance of each fixed effect using LRTs. As information theoretic approaches have become more popular, it is increasingly common to assess model fit by comparing the AIC values of different models, acknowledging the caveat that AIC can only tell us about the *relative* fit of alternative models, but not whether a particular model is a good fit, overall. Recall that AIC values are a way of inverting and scaling model log-likelihoods that penalizes models with greater numbers of parameters.

The `aictab()` function from {AICcmodavg} neatly prints out tables with AIC, Delta AIC, and log-likelihood values, along with AIC "weights".

```{r}
#| message: false
#| warning: false
library(AICcmodavg)
(aic_table <- aictab( list(full, minusRC, minusP, null),
  modnames = c("full", "minusRC", "minusP", "null")))
detach(package:AICcmodavg)
```

> **NOTE:** Here we are printing out AICc values, rather than AIC values. AICc is simply a version of AIC with a correction added for small sample sizes. To print the uncorrected AIC values, which appear in the `anova()` LRT output, we can add the argument `second.ord=TRUE` to the `aictab()` function.

$$AIC_c = AIC + \frac{2K^2 + 2K}{n-K-1}$$

where $K$ is the number of parameters and $n$ is the sample size.

In the table, for each model, $K$ is the number of model parameters, the $Delta\  AICc$ value is the difference between that model's AICc and the best model's AICc (again, here, that is the full model), and the $AICc\ weight$ is the relative likelihood of that model. [The weights for a particular set of models sums to 1, with each weight equal to the model's likelihood divided by the summed likelihoods across all models.]

Where the best model has a very high Akaike weight, e.g., \>0.9, it is reasonable to base inferences about the included variables on that single most parsimonious model, but when several models rank highly (e.g., several models have Delta AICc values \<2 to 4), it is common to model-average effect sizes for the variables that have the most support across that set of models. That is, "model averaging" means making inferences based on a **set** of candidate models, instead of on a single "best" model.

Here, note that the full model containing both *reproductive condition* and *parity* has the highest likelihood (the least negative log-likelihood value) and a much lower AICc than any of the less complex alternative models tested. It also has a very high Akaike weight (0.91).

> **NOTE:** When running a number of the above models and/or in doing the likelihood ratio tests, we saw a significant result but we also got either a warning that our null models "failed to converge" or we saw a warning about a "boundary (singular) fit". Both of these warning are due to having a LOT of parameters we are trying to estimate relative to the number of observations we have. Dealing with lack of convergence in fitting maximum likelihood models is beyond what we can cover here, but I encourage you to explore that on your own!

### Other Methods for Assessing Fit {.unnumbered}

Using likelihood ratio tests and AIC values for evaluating and expressing how well a particular model fits a dataset both have some critical limitations:

- While AIC provides an estimate of the *relative* fit of various models, it does not say anything about the absolute fit
- AIC does not address the amount of variance in a response variable explained by a model
- AIC is not comparable across datasets, and so fit is not generalizable

Nakagawa & Schielzeth (2013) and Nakagawa et al. (2017) have published a simple and effective method for calculating a type of *pseudo-*$R^2$ (or coefficient of determination) value for generalized linear mixed models, and because linear mixed models are a specific type of GLMM, this method can be used with LMMs as well.

> See:
>
> - Nakagawa, S., & Schielzeth, H. (2013). A general and simple method for obtaining R2 from generalized linear mixed-effects models. *Methods in Ecology and Evolution*, 4, 133--142.
>
> - Nakagawa, S., Johnson, P. C. D., & Schielzeth, H. (2017). The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded. *Journal of the Royal Society Interface*, 14, article 20170213.

In these papers, two measures are proposed for characterizing the amount of "variance explained" in for mixed effects models:

- $Marginal\ R2GLMM(m)^2$ is the amount of variance explained on the latent (or link) scale rather than the original scale. We can interpret this as a measure of the variance explained by only the fixed effects.
- $Conditional\ R2GLMM(c)^2$ is the amount of variance explained by both fixed and random effects, i.e., by the entire model.

There is an easy way to calculate these two R2GLMM values in ***R*** using the `r.squaredGLMM()` function in the package {MuMIn}.

### CHALLENGE {.unnumbered}

Compare the full, the two reduced, and the null mixed effects models from our random slope exercise using an information theoretic approach. Is the best model (full) the one that explains the greatest amount of variance in the dataset? In the full model, how much more of the total variance is explained by the random effects than by the fixed effects alone?

```{r}
#| message: false
#| warning: false
aic_table # re-print the AIC table
library(MuMIn)
r.squaredGLMM(full)
r.squaredGLMM(minusRC)
r.squaredGLMM(minusP)
r.squaredGLMM(null)
detach(package:MuMIn)
```

## Generalized LMMs

Just as we extended our standard linear modeling approach to include non-normally distributed response variables/error structures, so too can we extend our mixed effects modeling to such situations. This is referred to as **generalized linear mixed modeling, or GLMM**. There are several ***R*** packages we can use to do this under either a maximum likelihood (e.g., {lme4}, {glmmML}, the no-longer-maintained {glmmboot}) or Bayesian (e.g., {MCMCglmm}, {glmmTMB}, {brms}) framework. The methods for generating maximum likelihood and Bayesian parameter estimates under GLMMs are more complicated, but conceptually, the process is an extension of what we have talked about already. Below, we explore such a scenario.

### CHALLENGE {.unnumbered}

Boden-Parry et al. (2020) studied the effect of food type and abundance on the begging and food sharing behavior of otters in captivity.

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/Bowden-ParryOtterdata.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
library(glmmML)
```

Bowden-Parry, M.; Postma, E.; and Boogert, N.J. (2020). Effects of food type and abundance on begging and sharing in Asian small-clawed otters (*Aonyx cinereus*). *PeerJ* 8: e10369.

```{r}
#| include: false
detach(package:glmmML)
detach(package:lme4)
detach(package:tidyverse)
```

<hr>

## Concept Review {.unnumbered}

- "Mixed" modeling extends general and generalized linear modeling to consider cases where we have additional "random" factors that are another source of possible variation in our response variable
- The approach allows us to estimate either different intercepts or different slopes *and* intercepts for each level of the random factor
- As in generalized linear modeling, likelihood ratio tests and information criteria approaches can be used to compare the explanatory power of different models
- Though we do not address it in this class, regression modeling can be extended further to consider NONLINEAR relationships among predictor variables
  - The ***R*** packages {lme4} and {nlme} provide functions for performing for "nonlinear mixed effects modeling"
